{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slRRuoFcp4qA"
      },
      "outputs": [],
      "source": [
        "!pip install \"ray[tune]\" torch torchvision pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iX39Fpfxp2H4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "from filelock import FileLock\n",
        "from torchmetrics import Accuracy\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "\n",
        "from ray.train.lightning import LightningTrainer, LightningConfigBuilder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "95htc_dip2H8",
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "# If you want to run full test, please set SMOKE_TEST to False\n",
        "SMOKE_TEST = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfaRC2Oop2H9"
      },
      "source": [
        "Our example builds on the MNIST example from the [blog post](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09) we mentioned before. We adapted the original model and dataset definitions into `MNISTClassifier` and `MNISTDataModule`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V99QZ2d6WMtv"
      },
      "source": [
        "### Ligtning modules initiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uvrrHOZ8Wxrb"
      },
      "outputs": [],
      "source": [
        "from model.mlp import MLP\n",
        "from model.resnet import ResNet\n",
        "import torchvision.datasets as datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wAdyuB-pd0m0"
      },
      "outputs": [],
      "source": [
        "class MNISTDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size, train_transform=None, test_transform=None, image_size=None, train_valid_split=None):\n",
        "        super().__init__()\n",
        "        self.image_size = image_size if image_size is not None else 28\n",
        "        self.train_transform = train_transform if train_transform is not None else transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomRotation(30),\n",
        "            transforms.Resize(self.image_size),\n",
        "            transforms.CenterCrop(self.image_size),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        self.test_transform = test_transform if test_transform is not None else transforms.Compose([\n",
        "            transforms.Resize(self.image_size),\n",
        "            transforms.CenterCrop(self.image_size),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        self.batch_size = batch_size #config[\"batch_size\"]\n",
        "        self.train_valid_split = train_valid_split if train_valid_split is not None else 0.8\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        whole_train_dataset = datasets.FashionMNIST(root='data', train=True, transform=self.train_transform, download=True)\n",
        "        train_size = int(self.train_valid_split * len(whole_train_dataset))\n",
        "        valid_size = len(whole_train_dataset) - train_size\n",
        "        self.mnist_train, self.mnist_val = random_split(whole_train_dataset, [train_size, valid_size])\n",
        "        self.mnist_test = datasets.FashionMNIST(root='data', train=False, transform=self.test_transform, download=True)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=2)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ofd7aBnVp2H-"
      },
      "outputs": [],
      "source": [
        "class MNISTClassifier(pl.LightningModule):\n",
        "    def __init__(self, config):\n",
        "        super(MNISTClassifier, self).__init__()\n",
        "        self.accuracy = Accuracy('multiclass', num_classes=10)\n",
        "        self.mlp = MLP(**config[\"mlp_config\"])\n",
        "        self.resnet = ResNet(**config[\"resnet_config\"])\n",
        "        self.lr = config[\"lr\"]\n",
        "\n",
        "        self.validation_step_outputs = []\n",
        "\n",
        "    def cross_entropy_loss(self, logits, labels):\n",
        "        return F.nll_loss(logits, labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.resnet(x)\n",
        "        out = self.mlp(out)\n",
        "        return out\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        x, y = train_batch\n",
        "        logits = self.forward(x)\n",
        "        loss = self.cross_entropy_loss(logits, y)\n",
        "        accuracy = self.accuracy(logits, y)\n",
        "\n",
        "        self.log(\"ptl/train_loss\", loss)\n",
        "        self.log(\"ptl/train_accuracy\", accuracy)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        x, y = val_batch\n",
        "        logits = self.forward(x)\n",
        "        loss = self.cross_entropy_loss(logits, y)\n",
        "        accuracy = self.accuracy(logits, y)\n",
        "        self.validation_step_outputs.append({\"val_loss\": loss, \"val_accuracy\": accuracy})\n",
        "\n",
        "        return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        avg_loss = torch.stack([x[\"val_loss\"] for x in self.validation_step_outputs]).mean()\n",
        "        avg_acc = torch.stack([x[\"val_accuracy\"] for x in self.validation_step_outputs]).mean()\n",
        "        self.log(\"ptl/val_loss\", avg_loss)\n",
        "        self.log(\"ptl/val_accuracy\", avg_acc)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SwKjpLubp2H_"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "with open(\"model/configs/model.yaml\", 'r') as stream:\n",
        "      default_config=yaml.safe_load(stream)\n",
        "\n",
        "default_config = default_config['model']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84-3unIiblY9",
        "outputId": "ca0de8b6-41ad-4475-f7cd-42f73a04cc02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'regularization_ratio': 0.5,\n",
              " 'lr': 0.001,\n",
              " 'resnet_config': {'first_conv': {'in_channels': 3,\n",
              "   'out_channels': 64,\n",
              "   'kernel_size': 5,\n",
              "   'stride': 2,\n",
              "   'padding': 2},\n",
              "  'block_list': [{'in_channels': 64,\n",
              "    'out_channels': 64,\n",
              "    'kernel_size': 3,\n",
              "    'stride': 1,\n",
              "    'padding': 'same'},\n",
              "   {'in_channels': 64,\n",
              "    'out_channels': 128,\n",
              "    'kernel_size': 3,\n",
              "    'stride': 2,\n",
              "    'padding': 1},\n",
              "   {'in_channels': 128,\n",
              "    'out_channels': 128,\n",
              "    'kernel_size': 3,\n",
              "    'stride': 1,\n",
              "    'padding': 'same'},\n",
              "   {'in_channels': 128,\n",
              "    'out_channels': 256,\n",
              "    'kernel_size': 3,\n",
              "    'stride': 2,\n",
              "    'padding': 1},\n",
              "   {'in_channels': 256,\n",
              "    'out_channels': 256,\n",
              "    'kernel_size': 3,\n",
              "    'stride': 1,\n",
              "    'padding': 'same'}],\n",
              "  'pool_size': 2},\n",
              " 'mlp_config': {'block_list': [{'in_size': 1024,\n",
              "    'out_size': 512,\n",
              "    'activation_fun': 'relu',\n",
              "    'batch_norm': True,\n",
              "    'dropout': 0.0},\n",
              "   {'in_size': 512,\n",
              "    'out_size': 256,\n",
              "    'activation_fun': 'none',\n",
              "    'batch_norm': False,\n",
              "    'dropout': 0.0}]},\n",
              " 'img_key': 0,\n",
              " 'class_key': 1}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "default_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSOmWuAkp2IA"
      },
      "source": [
        "## Tuning the model parameters\n",
        "\n",
        "The parameters above should give you a good accuracy of over 90% already. However, we might improve on this simply by changing some of the hyperparameters. For instance, maybe we get an even higher accuracy if we used a smaller learning rate and larger middle layer size.\n",
        "\n",
        "Instead of manually loop through all the parameter combinitions, let's use Tune to systematically try out parameter combinations and find the best performing set.\n",
        "\n",
        "First, we need some additional imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xWfxuIaYp2IA"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from ray import air, tune\n",
        "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
        "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jwXz8T8p2IB"
      },
      "source": [
        "### Configuring the search space\n",
        "\n",
        "Now we configure the parameter search space using {class}`LightningConfigBuilder <ray.train.lightning.LightningConfigBuilder>`. We would like to choose between three different layer and batch sizes. The learning rate should be sampled uniformly between `0.0001` and `0.1`. The `tune.loguniform()` function is syntactic sugar to make sampling between these different orders of magnitude easier, specifically we are able to also sample small values.\n",
        "\n",
        ":::{note}\n",
        "In `LightningTrainer`, the frequency of metric reporting is the same as the frequency of checkpointing. For example, if you set `builder.checkpointing(..., every_n_epochs=2)`, then for every 2 epochs, all the latest metrics will be reported to the Ray Tune session along with the latest checkpoint. Please make sure the target metrics(e.g. metrics specified in `TuneConfig`, schedulers, and searchers) are logged before saving a checkpoint.\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        ":::{note}\n",
        "Use `LightningConfigBuilder.checkpointing()` to specify the monitor metric and checkpoint frequency for the Lightning ModelCheckpoint callback. To properly save AIR checkpoints, you must also provide an AIR {class}`CheckpointConfig <ray.air.config.CheckpointConfig>`. Otherwise, LightningTrainer will create a default CheckpointConfig, which saves all the reported checkpoints by default.\n",
        "\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "i_F39Mz_p2IB"
      },
      "outputs": [],
      "source": [
        "# The maximum training epochs\n",
        "num_epochs = 5\n",
        "\n",
        "# Number of sampls from parameter space\n",
        "num_samples = 10\n",
        "\n",
        "accelerator = \"gpu\"\n",
        "\n",
        "config = {\n",
        "    'lr': tune.choice([0.001, 0.01]),\n",
        "    #'batch_size': tune.choice([64, 128, 256]),\n",
        "    'resnet_config':\n",
        "     {\n",
        "      'first_conv':\n",
        "        {\n",
        "          'in_channels': 1,\n",
        "          'out_channels': 32,\n",
        "          'kernel_size': 3,\n",
        "          'stride': 2,\n",
        "          'padding': 1\n",
        "        },\n",
        "      'block_list': [\n",
        "        {\n",
        "          'in_channels': 32,\n",
        "          'out_channels': 16,\n",
        "          'kernel_size': 3,\n",
        "          'stride': 2,\n",
        "          'padding': 1\n",
        "        },\n",
        "        {\n",
        "          'in_channels': 16,\n",
        "          'out_channels': 8,\n",
        "          'kernel_size': 3,\n",
        "          'stride': 1,\n",
        "          'padding': 'same'\n",
        "        }\n",
        "      ],\n",
        "      'pool_size': 2\n",
        "    },\n",
        "    'mlp_config':\n",
        "     {\n",
        "        'block_list': [\n",
        "        {\n",
        "          'in_size': 72,\n",
        "          'out_size': 64,\n",
        "          'activation_fun': 'relu',\n",
        "          'batch_norm': True,\n",
        "          'dropout': 0.1\n",
        "        },\n",
        "        {\n",
        "          'in_size': 64,\n",
        "          'out_size': 10,\n",
        "          'activation_fun': 'logsoftmax',\n",
        "          'batch_norm': False,\n",
        "          'dropout': 0.0\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    'img_key': 0,\n",
        "    'class_key': 1\n",
        "  }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N_96hGup2IC",
        "tags": []
      },
      "source": [
        "If you have more resources available, you can modify the above parameters accordingly. e.g. more epochs, more parameter samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "j-L-XlfUp2ID",
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "if SMOKE_TEST:\n",
        "    num_epochs = 3\n",
        "    num_samples = 3\n",
        "    accelerator = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "s75QZ0G8p2ID"
      },
      "outputs": [],
      "source": [
        "dm = MNISTDataModule(64)\n",
        "logger = TensorBoardLogger(save_dir=os.getcwd(), name=\"tune-mnist\", version=\".\")\n",
        "\n",
        "lightning_config = (\n",
        "    LightningConfigBuilder()\n",
        "    .module(cls=MNISTClassifier, config=config)\n",
        "    .trainer(max_epochs=num_epochs, accelerator=accelerator, logger=logger)\n",
        "    .fit_params(datamodule=dm)\n",
        "    .checkpointing(monitor=\"ptl/val_accuracy\", save_top_k=2, mode=\"max\")\n",
        "    .build()\n",
        ")\n",
        "\n",
        "# Make sure to also define an AIR CheckpointConfig here\n",
        "# to properly save checkpoints in AIR format.\n",
        "run_config = RunConfig(\n",
        "    checkpoint_config=CheckpointConfig(\n",
        "        num_to_keep=2,\n",
        "        checkpoint_score_attribute=\"ptl/val_accuracy\",\n",
        "        checkpoint_score_order=\"max\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2AIQ-N5p2IE"
      },
      "source": [
        "### Selecting a scheduler\n",
        "\n",
        "In this example, we use an [Asynchronous Hyperband](https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/)\n",
        "scheduler. This scheduler decides at each iteration which trials are likely to perform\n",
        "badly, and stops these trials. This way we don't waste any resources on bad hyperparameter\n",
        "configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "K1HhhOHep2IE"
      },
      "outputs": [],
      "source": [
        "scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRduQN9fp2IF"
      },
      "source": [
        "### Training with GPUs\n",
        "\n",
        "We can specify the number of resources, including GPUs, that Tune should request for each trial.\n",
        "\n",
        "`LightningTrainer` takes care of environment setup for Distributed Data Parallel training, the model and data will automatically get distributed across GPUs. You only need to set the number of GPUs per worker in `ScalingConfig` and also set `accelerator=\"gpu\"` in LightningTrainerConfigBuilder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "g9gLzaqjp2IF"
      },
      "outputs": [],
      "source": [
        "scaling_config = ScalingConfig(\n",
        "    num_workers=2, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "qSKhOSbFp2IG",
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "if SMOKE_TEST:\n",
        "    scaling_config = ScalingConfig(\n",
        "        num_workers=1, use_gpu=False, resources_per_worker={\"CPU\": 1}\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xXh4PU1Zp2IG"
      },
      "outputs": [],
      "source": [
        "# Define a base LightningTrainer without hyper-parameters for Tuner\n",
        "lightning_trainer = LightningTrainer(\n",
        "    scaling_config=scaling_config,\n",
        "    run_config=run_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g07_qJ4Np2IG"
      },
      "source": [
        "### Putting it together\n",
        "\n",
        "Lastly, we need to create a `Tuner()` object and start Ray Tune with `tuner.fit()`.\n",
        "\n",
        "The full code looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Lm8r6nbp2IH"
      },
      "outputs": [],
      "source": [
        "def tune_mnist_asha(num_samples=10):\n",
        "    scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n",
        "\n",
        "    tuner = tune.Tuner(\n",
        "        lightning_trainer,\n",
        "        param_space={\"lightning_config\": lightning_config},\n",
        "        tune_config=tune.TuneConfig(\n",
        "            metric=\"ptl/val_accuracy\",\n",
        "            mode=\"max\",\n",
        "            num_samples=num_samples,\n",
        "            scheduler=scheduler,\n",
        "        ),\n",
        "        run_config=air.RunConfig(\n",
        "            name=\"tune_mnist_asha\",\n",
        "        ),\n",
        "    )\n",
        "    results = tuner.fit()\n",
        "    best_result = results.get_best_result(metric=\"ptl/val_accuracy\", mode=\"max\")\n",
        "\n",
        "\n",
        "tune_mnist_asha(num_samples=num_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuE2XZE3p2II"
      },
      "source": [
        "## Using Population Based Training to find the best parameters\n",
        "\n",
        "The `ASHAScheduler` terminates those trials early that show bad performance.\n",
        "Sometimes, this stops trials that would get better after more training steps,\n",
        "and which might eventually even show better performance than other configurations.\n",
        "\n",
        "Another popular method for hyperparameter tuning, called\n",
        "[Population Based Training](https://deepmind.com/blog/article/population-based-training-neural-networks),\n",
        "instead perturbs hyperparameters during the training run. Tune implements PBT, and\n",
        "we only need to make some slight adjustments to our code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmwHzUTjp2IJ"
      },
      "outputs": [],
      "source": [
        "def tune_mnist_pbt(num_samples=10):\n",
        "    # The range of hyperparameter perturbation.\n",
        "    mutations_config = (\n",
        "        LightningConfigBuilder()\n",
        "        .module(\n",
        "            config={\n",
        "                \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "            }\n",
        "        )\n",
        "        .build()\n",
        "    )\n",
        "\n",
        "    # Create a PBT scheduler\n",
        "    scheduler = PopulationBasedTraining(\n",
        "        perturbation_interval=1,\n",
        "        time_attr=\"training_iteration\",\n",
        "        hyperparam_mutations={\"lightning_config\": mutations_config},\n",
        "    )\n",
        "\n",
        "    tuner = tune.Tuner(\n",
        "        lightning_trainer,\n",
        "        param_space={\"lightning_config\": lightning_config},\n",
        "        tune_config=tune.TuneConfig(\n",
        "            metric=\"ptl/val_accuracy\",\n",
        "            mode=\"max\",\n",
        "            num_samples=num_samples,\n",
        "            scheduler=scheduler,\n",
        "        ),\n",
        "        run_config=air.RunConfig(\n",
        "            name=\"tune_mnist_pbt\",\n",
        "        ),\n",
        "    )\n",
        "    results = tuner.fit()\n",
        "    best_result = results.get_best_result(metric=\"ptl/val_accuracy\", mode=\"max\")\n",
        "    best_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBUFunSGp2IJ"
      },
      "outputs": [],
      "source": [
        "tune_mnist_pbt(num_samples=num_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_fd4ncqp2IK"
      },
      "source": [
        "An example output of a run could look like this:\n",
        "\n",
        "```bash\n",
        ":emphasize-lines: 12\n",
        "\n",
        " +------------------------------+------------+-------+----------------+----------------+---------------------+-----------+--------------------+----------------------+\n",
        " | Trial name                   | status     | loc   |   layer_1_size |   layer_2_size |                  lr |      loss |   ptl/val_accuracy |   training_iteration |\n",
        " |------------------------------+------------+-------+----------------+----------------+---------------------+-----------+--------------------+----------------------|\n",
        " | LightningTrainer_85489_00000 | TERMINATED |       |            64  |            64  | 0.0030@perturbed... | 0.108734  |        0.984954    |                   5  |\n",
        " | LightningTrainer_85489_00001 | TERMINATED |       |            32  |            256 | 0.0010@perturbed... | 0.093577  |        0.983411    |                   5  |\n",
        " | LightningTrainer_85489_00002 | TERMINATED |       |            128 |            64  | 0.0233@perturbed... | 0.0922348 |        0.983989    |                   5  |\n",
        " | LightningTrainer_85489_00003 | TERMINATED |       |            64  |            128 | 0.0002@perturbed... | 0.124648  |        0.98206\t  |                   5  |\n",
        " | LightningTrainer_85489_00004 | TERMINATED |       |            128 |            256 | 0.0021              | 0.101717  |        0.993248    |                   5  |\n",
        " | LightningTrainer_85489_00005 | TERMINATED |       |            32  |            128 | 0.0003@perturbed... | 0.121467  |        0.984182    |                   5  |\n",
        " | LightningTrainer_85489_00006 | TERMINATED |       |            128 |            64  | 0.0020@perturbed... | 0.053446  |        0.984375    |                   5  |\n",
        " | LightningTrainer_85489_00007 | TERMINATED |       |            64  |            64  | 0.0063@perturbed... | 0.129804  |        0.98669\t  |                   5  |\n",
        " | LightningTrainer_85489_00008 | TERMINATED |       |            128 |            256 | 0.0436@perturbed... | 0.363236  |        0.982253    |                   5  |\n",
        " | LightningTrainer_85489_00009 | TERMINATED |       |            128 |            256 | 0.001               | 0.150946  |        0.985147    |                   5  |\n",
        " +------------------------------+------------+-------+----------------+----------------+---------------------+-----------+--------------------+----------------------+\n",
        "```\n",
        "\n",
        "As you can see, each sample ran the full number of 5 iterations.\n",
        "All trials ended with quite good parameter combinations and showed relatively good performances (above `0.98`).\n",
        "In some runs, the parameters have been perturbed. And the best configuration even reached a mean validation accuracy of `0.993248`!\n",
        "\n",
        "In summary, AIR LightningTrainer is easy to extend to use with Tune. It only required adding a few lines of code to integrate with Ray Tuner to get great performing parameter configurations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcDj0VGHp2IK"
      },
      "source": [
        "## More PyTorch Lightning Examples\n",
        "\n",
        "- {ref}`Use LightningTrainer for Image Classification <lightning_mnist_example>`.\n",
        "- {ref}`Use LightningTrainer with Ray Data and Batch Predictor <lightning_advanced_example>`\n",
        "- {ref}`Fine-tune a Large Language Model with LightningTrainer and FSDP <dolly_lightning_fsdp_finetuning>`\n",
        "- {doc}`/tune/examples/includes/mlflow_ptl_example`: Example for using [MLflow](https://github.com/mlflow/mlflow/)\n",
        "  and [Pytorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) with Ray Tune.\n",
        "- {doc}`/tune/examples/includes/mnist_ptl_mini`:\n",
        "  A minimal example of using [Pytorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning)\n",
        "  to train a MNIST model. This example utilizes the Ray Tune-provided\n",
        "  {ref}`PyTorch Lightning callbacks <tune-integration-pytorch-lightning>`.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
